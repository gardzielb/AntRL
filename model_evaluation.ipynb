{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\gymnasium\\envs\\registration.py:498: UserWarning: \u001B[33mWARN: Overriding environment GymV26Environment-v0 already in registry.\u001B[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium\n",
    "import sys\n",
    "\n",
    "sys.modules[\"gym\"] = gymnasium\n",
    "from stable_baselines3.sac import SAC\n",
    "from sb3_contrib.ars import ARS\n",
    "\n",
    "from antrl.experiments import test_algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Efficiency and training time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ARS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[?25l",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8d661207fd34488bdeded0cfb99b875"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 12\u001B[0m\n\u001B[0;32m      1\u001B[0m ars_env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAnt-v4\u001B[39m\u001B[38;5;124m'\u001B[39m, healthy_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.95\u001B[39m)\n\u001B[0;32m      2\u001B[0m ars \u001B[38;5;241m=\u001B[39m ARS(\n\u001B[0;32m      3\u001B[0m \t\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLinearPolicy\u001B[39m\u001B[38;5;124m'\u001B[39m, ars_env,\n\u001B[0;32m      4\u001B[0m \talive_bonus_offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      9\u001B[0m \tdevice \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     10\u001B[0m )\n\u001B[1;32m---> 12\u001B[0m ars_result \u001B[38;5;241m=\u001B[39m \u001B[43mtest_algorithm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1_000_000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Devel\\Repos\\PycharmProjects\\AntRL\\antrl\\experiments.py:25\u001B[0m, in \u001B[0;36mtest_algorithm\u001B[1;34m(algorithm, n_epochs, n_eval_episodes, save_algorithm)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_algorithm\u001B[39m(\n\u001B[0;32m     22\u001B[0m \t\talgorithm: BaseAlgorithm, n_epochs: \u001B[38;5;28mint\u001B[39m, n_eval_episodes: \u001B[38;5;28mint\u001B[39m, save_algorithm: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     23\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m TestResult:\n\u001B[0;32m     24\u001B[0m \tstart_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[1;32m---> 25\u001B[0m \t\u001B[43malgorithm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mn_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \ttraining_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n\u001B[0;32m     28\u001B[0m \t\u001B[38;5;28;01mif\u001B[39;00m save_algorithm:\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\sb3_contrib\\ars\\ars.py:357\u001B[0m, in \u001B[0;36mARS.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, eval_env, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, async_eval, progress_bar)\u001B[0m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_steps:\n\u001B[0;32m    356\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_current_progress_remaining(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps, total_timesteps)\n\u001B[1;32m--> 357\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_one_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43masync_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m log_interval \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_updates \u001B[38;5;241m%\u001B[39m log_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    359\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_and_dump()\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\sb3_contrib\\ars\\ars.py:276\u001B[0m, in \u001B[0;36mARS._do_one_update\u001B[1;34m(self, callback, async_eval)\u001B[0m\n\u001B[0;32m    273\u001B[0m candidate_weights \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mcat([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m+\u001B[39m policy_deltas, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m-\u001B[39m policy_deltas])\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 276\u001B[0m     candidate_returns \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcandidate_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43masync_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;66;03m# Returns corresponding to weights + deltas\u001B[39;00m\n\u001B[0;32m    279\u001B[0m plus_returns \u001B[38;5;241m=\u001B[39m candidate_returns[: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_delta]\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\sb3_contrib\\ars\\ars.py:220\u001B[0m, in \u001B[0;36mARS.evaluate_candidates\u001B[1;34m(self, candidate_weights, callback, async_eval)\u001B[0m\n\u001B[0;32m    218\u001B[0m train_policy\u001B[38;5;241m.\u001B[39mload_from_vector(candidate_weights[weights_idx]\u001B[38;5;241m.\u001B[39mcpu())\n\u001B[0;32m    219\u001B[0m \u001B[38;5;66;03m# Evaluate the candidate\u001B[39;00m\n\u001B[1;32m--> 220\u001B[0m episode_rewards, episode_lengths \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_policy\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    222\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_eval_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_eval_episodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_episode_rewards\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Increment num_timesteps too (slight mismatch with multi envs)\u001B[39;49;00m\n\u001B[0;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_trigger_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_envs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_envs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    228\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;66;03m# Update reward to cancel out alive bonus if needed\u001B[39;00m\n\u001B[0;32m    230\u001B[0m candidate_returns[weights_idx] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(episode_rewards) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malive_bonus_offset \u001B[38;5;241m*\u001B[39m \u001B[38;5;28msum\u001B[39m(episode_lengths)\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001B[0m, in \u001B[0;36mevaluate_policy\u001B[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001B[0m\n\u001B[0;32m     86\u001B[0m episode_starts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones((env\u001B[38;5;241m.\u001B[39mnum_envs,), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m (episode_counts \u001B[38;5;241m<\u001B[39m episode_count_targets)\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m---> 88\u001B[0m     actions, states \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m     90\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[43m        \u001B[49m\u001B[43mepisode_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepisode_starts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     92\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdeterministic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdeterministic\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     94\u001B[0m     observations, rewards, dones, infos \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n\u001B[0;32m     95\u001B[0m     current_rewards \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m rewards\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:324\u001B[0m, in \u001B[0;36mBasePolicy.predict\u001B[1;34m(self, observation, state, episode_start, deterministic)\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# TODO (GH/1): add support for RNN policies\u001B[39;00m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;66;03m# if state is None:\u001B[39;00m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;66;03m#     state = self.initial_state\u001B[39;00m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;66;03m# if episode_start is None:\u001B[39;00m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;66;03m#     episode_start = [False for _ in range(self.n_envs)]\u001B[39;00m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001B[39;00m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_training_mode(\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 324\u001B[0m observation, vectorized_env \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobs_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    327\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predict(observation, deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\stable_baselines3\\common\\policies.py:243\u001B[0m, in \u001B[0;36mBaseModel.obs_to_tensor\u001B[1;34m(self, observation)\u001B[0m\n\u001B[0;32m    240\u001B[0m     \u001B[38;5;66;03m# Add batch dimension if needed\u001B[39;00m\n\u001B[0;32m    241\u001B[0m     observation \u001B[38;5;241m=\u001B[39m observation\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m--> 243\u001B[0m observation \u001B[38;5;241m=\u001B[39m \u001B[43mobs_as_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m observation, vectorized_env\n",
      "File \u001B[1;32mc:\\users\\barto\\devel\\repos\\pycharmprojects\\antrl\\venv\\lib\\site-packages\\stable_baselines3\\common\\utils.py:465\u001B[0m, in \u001B[0;36mobs_as_tensor\u001B[1;34m(obs, device)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;124;03mMoves the observation to the given device.\u001B[39;00m\n\u001B[0;32m    459\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;124;03m:return: PyTorch tensor of the observation on a desired device.\u001B[39;00m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obs, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m--> 465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mth\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    466\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obs, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    467\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {key: th\u001B[38;5;241m.\u001B[39mas_tensor(_obs)\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m (key, _obs) \u001B[38;5;129;01min\u001B[39;00m obs\u001B[38;5;241m.\u001B[39mitems()}\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ars_env = gym.make('Ant-v4', healthy_reward = 0.95)\n",
    "ars = ARS(\n",
    "\t'LinearPolicy', ars_env,\n",
    "\talive_bonus_offset = -1,\n",
    "\tdelta_std = 0.025,\n",
    "\tlearning_rate = 0.015,\n",
    "\tn_delta = 60,\n",
    "\tn_top = 20,\n",
    "\tdevice = 'cuda'\n",
    ")\n",
    "\n",
    "ars_result = test_algorithm(ars, n_epochs = 1_000_000, n_eval_episodes = 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sac_env = gym.make('Ant-v4', healthy_reward = 0.01)\n",
    "sac = SAC(\"MlpPolicy\", ars_env, learning_starts = 10_000)\n",
    "sac_result = test_algorithm(sac, n_epochs = 100_000, n_eval_episodes = 5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
